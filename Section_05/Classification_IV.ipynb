{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgFLM7jqMvzM"
      },
      "source": [
        "# Section 05: Classification IV\n",
        "\n",
        "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.txt\"><img alt=\"Attribution-NonCommercial-ShareAlike 4.0 International\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by-nc-sa.eu.svg\" title=\"This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Qyp0ovl-Xv"
      },
      "source": [
        "End-to-end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import *\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### define some metrics which are shown during the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Neural Regression\n",
        "#### The following function 'emulates' logistic regression as an artificial neural network in the simplest form possible\n",
        "#### for this simple network (just one input), the keras/tensorflow-sequential-API can be used which expects all layers in sequential order\n",
        "#### however, for 'emulating' logistic regression, there is only one layer with a sigmoid activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_neural_model(metrics=METRICS):\n",
        "\n",
        "  model = keras.Sequential([\n",
        "      #in principle, additional layers can be added - however, it is unlikely that for the current features (and without additional techniques like normalization etc.) results would improve significantly\n",
        "      #keras.layers.Dense(128, activation=\"relu\"),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and inference can be done analogously to 'usual' logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_logistic_ANN(feature_file, model_file, epochs=20, batch_size=32):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Function for training of the logistic neural regression using keras/tensorflow\n",
        "\n",
        "    Arguments:\n",
        "        feature_file str: Filename of the feature file for training.\n",
        "        model_file str: Filename of the output model.\n",
        "        epochs int: number of epochs, i.e. how often the full training data set is processed during training\n",
        "        batch_size int: batch size number, i.e. how many instances are processed simultaneously\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    feature_df = pd.read_csv(feature_file, sep=\"\\t\")\n",
        "\n",
        "    model = log_neural_model()\n",
        "\n",
        "\n",
        "    # Remove feature vectors with label -1\n",
        "    feature_df = feature_df[feature_df['label'] != -1]\n",
        "    labels = feature_df['label']\n",
        "    data = feature_df.drop(\n",
        "        columns=['chrom', 'start', 'end', 'sample', 'label'])\n",
        "\n",
        "    #split data in train and validation/test data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1)\n",
        "\n",
        "    #train the model\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "    os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "    pickle.dump(model, open(model_file, \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer_logistic_ANN(feature_file, model_file):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Function for inference using logistic neural regression implemented in keras/tensorflow\n",
        "\n",
        "    Arguments:\n",
        "        feature_file str: Filename of the feature file for inference.\n",
        "        model_file str: Filename of the trained ANN.\n",
        "    \"\"\"\n",
        "    with open(model_file, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    feature_df = pd.read_csv(feature_file, sep=\"\\t\")\n",
        "    data = feature_df.drop(columns=['chrom', 'start', 'end', 'sample'])\n",
        "\n",
        "    predictions = model.predict(data)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Neural Regression with additional feature-processing layers\n",
        "#### in this ANN, some of the features run through additional layers before all of them are merged for the final logistic regression step\n",
        "#### In this case, as multiple inputs are given, we have to use the slightly more complicated functional API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_neural_model_extended(feature_df_tons, feature_df_pairwise, feature_df_rest, metrics=METRICS, cnn = False):\n",
        "    from keras.layers import Input, Concatenate, Flatten, Dense, Conv1D\n",
        "    from keras.models import Model\n",
        "  \n",
        "    #determining the shape of the inputs\n",
        "    tons_input = Input((feature_df_tons.shape[1],1))\n",
        "    pairwise_input=Input((feature_df_pairwise.shape[1],1))\n",
        "    rest_input = Input((feature_df_rest.shape[1],))\n",
        "\n",
        "    if cnn == True:\n",
        "\n",
        "        conv_layer1 = Conv1D(filters=32, kernel_size=5)(tons_input)\n",
        "        flat_layer1 = Flatten()(conv_layer1)\n",
        "\n",
        "        conv_layer2 = Conv1D(filters=32, kernel_size=5)(pairwise_input)\n",
        "        flat_layer2 = Flatten()(conv_layer2)\n",
        "\n",
        "        # Concatenate the convolutional features and the vector input\n",
        "        concat_layer= Concatenate()([flat_layer1, flat_layer2, rest_input])\n",
        "\n",
        "    else:\n",
        "        dense_layer1 = Dense(4, activation=\"relu\")(tons_input)\n",
        "        flat_layer1 = Flatten()(dense_layer1)\n",
        "\n",
        "        dense_layer2 = Dense(4, activation=\"relu\")(pairwise_input)\n",
        "        flat_layer2 = Flatten()(dense_layer2)\n",
        "\n",
        "        # Concatenate the dense layers and the vector input\n",
        "        concat_layer= Concatenate()([flat_layer1, flat_layer2, rest_input])\n",
        "\n",
        "    #final neural logistic regression / sigmoid layer\n",
        "    output = keras.layers.Dense(1, activation=\"sigmoid\")(concat_layer)\n",
        "\n",
        "    model = Model(inputs=[tons_input, pairwise_input, rest_input], outputs=output)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def partition_feature_df(feature_df):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Function which partitions the original dataframe into three dataframes depending on the name of the features\n",
        "\n",
        "    Arguments:\n",
        "        feature_df Pd.Dataframe: Pandas dataframe containing the features of the training/test data\n",
        "    \"\"\"\n",
        "\n",
        "    pairwise_cols = [col for col in feature_df.columns if ( col.startswith(\"pairwised_dist\"))]\n",
        "    ton_cols = [col for col in feature_df.columns if ('-ton' in col )]\n",
        "    dynamic_cols = [col for col in feature_df.columns if ('-ton' in col or col.startswith(\"pairwised_dist\"))]\n",
        "\n",
        "\n",
        "    feature_df_tons = feature_df[ton_cols]\n",
        "    feature_df_pairwise = feature_df[pairwise_cols]\n",
        "    feature_df_rest = feature_df.drop(dynamic_cols, axis=1, inplace = False, errors='ignore')\n",
        "\n",
        "    return feature_df_tons, feature_df_pairwise, feature_df_rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_logistic_ANN_extended(feature_file, model_file, epochs=20, batch_size=32):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Function for training of the extended logistic neural regression model implemented in keras/tensorflow\n",
        "\n",
        "    Arguments:\n",
        "        feature_file str: Filename of the feature file for training.\n",
        "        model_file str: Filename of the output model.\n",
        "        epochs int: number of epochs, i.e. how often the full training data set is processed during training\n",
        "        batch_size int: batch size number, i.e. how many instances are processed simultaneously\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    feature_df = pd.read_csv(feature_file, sep=\"\\t\")\n",
        "\n",
        "    model = log_neural_model_extended()\n",
        "\n",
        "\n",
        "    # Remove feature vectors with label -1\n",
        "    feature_df = feature_df[feature_df['label'] != -1]\n",
        "    labels = feature_df['label']\n",
        "\n",
        "    data = feature_df.drop(\n",
        "        columns=['chrom', 'start', 'end', 'sample', 'label'])\n",
        "\n",
        "    #split data in train and validation/test data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1)\n",
        "\n",
        "    #for this architecture, we have to separate the data frame in different inputs\n",
        "    X_train_tons, X_train_pairwise, X_train_rest = partition_feature_df(X_train)\n",
        "    X_test_tons, X_test_pairwise, X_test_rest = partition_feature_df(X_test)\n",
        "\n",
        "    #train the model\n",
        "    model.fit([X_train_tons, X_train_pairwise, X_train_rest], y_train, epochs=epochs, batch_size=batch_size, validation_data=([X_test_tons, X_test_pairwise, X_test_rest], y_test))\n",
        "\n",
        "    os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "    pickle.dump(model, open(model_file, \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer_logistic_ANN_extended(feature_file, model_file):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    Function for inference using the extended logistic neural regression model implemented in keras/tensorflow\n",
        "\n",
        "    Arguments:\n",
        "        feature_file str: Filename of the feature file for inference.\n",
        "        model_file str: Filename of the ANN.\n",
        "    \"\"\"\n",
        "    with open(model_file, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    feature_df = pd.read_csv(feature_file, sep=\"\\t\")\n",
        "\n",
        "\n",
        "    data = feature_df.drop(columns=['chrom', 'start', 'end', 'sample'])\n",
        "\n",
        "    feature_df_tons, feature_df_pairwise, feature_df_rest = partition_feature_df(data)\n",
        "\n",
        "    predictions = model.predict([feature_df_tons, feature_df_pairwise, feature_df_rest])\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### the results can - if the true labels of the test data set are available - evaluated by means of precision-recall curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_precision_recall_curve(predictions, y_test, title=\"logistic neural network\"):\n",
        "    from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test.astype(int), predictions)\n",
        "    plt.plot(recall, precision, marker='.', label=title)\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "newest_tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
